{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4657e73880534f44bea376de633672a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1606733001087_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-27-37.us-west-2.compute.internal:20888/proxy/application_1606733001087_0008/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-18-64.us-west-2.compute.internal:8042/node/containerlogs/container_1606733001087_0008_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raiders of the lost kek\n",
    "\n",
    "The dataset comes from an academic research project. It is semi-structured, that is it has a structure but it is also nested. The code below opens the raw file (already on S3) and shows the schema it found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70af885d574c4736b2b462111d64a253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- posts: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- archived: long (nullable = true)\n",
      " |    |    |-- archived_on: long (nullable = true)\n",
      " |    |    |-- bumplimit: long (nullable = true)\n",
      " |    |    |-- capcode: string (nullable = true)\n",
      " |    |    |-- closed: long (nullable = true)\n",
      " |    |    |-- com: string (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- country_name: string (nullable = true)\n",
      " |    |    |-- entities: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- entity_end: long (nullable = true)\n",
      " |    |    |    |    |-- entity_label: string (nullable = true)\n",
      " |    |    |    |    |-- entity_start: long (nullable = true)\n",
      " |    |    |    |    |-- entity_text: string (nullable = true)\n",
      " |    |    |-- entitites: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- ext: string (nullable = true)\n",
      " |    |    |-- extracted_poster_id: string (nullable = true)\n",
      " |    |    |-- filedeleted: long (nullable = true)\n",
      " |    |    |-- filename: string (nullable = true)\n",
      " |    |    |-- fsize: long (nullable = true)\n",
      " |    |    |-- h: long (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- imagelimit: long (nullable = true)\n",
      " |    |    |-- images: long (nullable = true)\n",
      " |    |    |-- m_img: long (nullable = true)\n",
      " |    |    |-- md5: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- no: long (nullable = true)\n",
      " |    |    |-- now: string (nullable = true)\n",
      " |    |    |-- perspectives: string (nullable = true)\n",
      " |    |    |-- replies: long (nullable = true)\n",
      " |    |    |-- resto: long (nullable = true)\n",
      " |    |    |-- semantic_url: string (nullable = true)\n",
      " |    |    |-- since4pass: string (nullable = true)\n",
      " |    |    |-- spoiler: long (nullable = true)\n",
      " |    |    |-- sticky: long (nullable = true)\n",
      " |    |    |-- sub: string (nullable = true)\n",
      " |    |    |-- tail_size: long (nullable = true)\n",
      " |    |    |-- tim: long (nullable = true)\n",
      " |    |    |-- time: long (nullable = true)\n",
      " |    |    |-- tn_h: long (nullable = true)\n",
      " |    |    |-- tn_w: long (nullable = true)\n",
      " |    |    |-- trip: string (nullable = true)\n",
      " |    |    |-- troll_country: string (nullable = true)\n",
      " |    |    |-- unique_ips: long (nullable = true)\n",
      " |    |    |-- w: long (nullable = true)\n",
      " |    |    |-- xa18: long (nullable = true)\n",
      " |    |    |-- xa19l: long (nullable = true)\n",
      " |    |    |-- xa19s: long (nullable = true)\n",
      " |    |    |-- xh17: long (nullable = true)\n",
      " |    |    |-- xh17c: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "input_bucket = 's3://raiders-lost-kek-us-west-2'\n",
    "input_path = '/pol_062016-112019_labeled/pol_062016-112019_labeled.ndjson'\n",
    "df = spark.read.json(input_bucket + input_path)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrangle the data a bit by \"remembering\" the original post for each thread, and also having a measure of thread size available at each post level. Let's convert the array of posts into separate rows, one for each thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbbd3dd5a064a6bbc46ecfcea4cc13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = df.withColumn(\"ori_post\", df.posts[0].no).withColumn(\"replies_top\", df.posts[0].replies+1)\n",
    "df3 = df2.select(\"ori_post\",\n",
    "                 \"replies_top\",\n",
    "                 explode(df2.posts).alias(\"posts_exp\")\n",
    "                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, here is a list of desirable columns from the data dictionary document, which I converted from PDF to XLS. The prefix 'posts_exp' is there to account for the field name above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68446be9b29d42e6830d8ca59a2e7901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_select = ['posts_exp.no',\n",
    " 'posts_exp.resto',\n",
    " 'posts_exp.sticky',\n",
    " 'posts_exp.closed',\n",
    " 'posts_exp.now',\n",
    " 'posts_exp.time',\n",
    " 'posts_exp.name',\n",
    " 'posts_exp.trip',\n",
    " 'posts_exp.id',\n",
    " 'posts_exp.capcode',\n",
    " 'posts_exp.country',\n",
    " 'posts_exp.country_name',\n",
    " 'posts_exp.sub',\n",
    " 'posts_exp.com',\n",
    " 'posts_exp.tim',\n",
    " 'posts_exp.filename',\n",
    " 'posts_exp.ext',\n",
    " 'posts_exp.fsize',\n",
    " 'posts_exp.md5',\n",
    " 'posts_exp.w',\n",
    " 'posts_exp.h',\n",
    " 'posts_exp.tn_w',\n",
    " 'posts_exp.tn_h',\n",
    " 'posts_exp.filedeleted',\n",
    " 'posts_exp.spoiler',\n",
    " 'posts_exp.custom_spoiler',\n",
    " 'posts_exp.replies',\n",
    " 'posts_exp.images',\n",
    " 'posts_exp.bumplimit',\n",
    " 'posts_exp.imagelimit',\n",
    " 'posts_exp.semantic_url',\n",
    " 'posts_exp.since4pass',\n",
    " 'posts_exp.unique_ips',\n",
    " 'posts_exp.m_img',\n",
    " 'posts_exp.archived',\n",
    " 'posts_exp.archived_on',\n",
    " 'posts_exp.extracted_poster_id',\n",
    " 'posts_exp.troll_country']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make sure our final dataframe contains only valid columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0aaf4897094c739a0402358ed6c5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts_exp.custom_spoiler not found in schema."
     ]
    }
   ],
   "source": [
    "my_new_select = []\n",
    "for c in my_select:\n",
    "    try:\n",
    "        df3.select(c)\n",
    "        my_new_select.append(c)\n",
    "    except:\n",
    "        print(f\"{c} not found in schema.\")\n",
    "my_final_select = [\"ori_post\", \"replies_top\"] + my_new_select + [\"posts_exp.perspectives\", \"posts_exp.entities\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add partitioning to the data, so that people can run query against individual years / months and scan faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4f78681f02436b8d8228393fb33dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df4 = (df3.select(my_final_select)\n",
    "       .withColumn(\"full_date\", (from_unixtime(\"time\")))\n",
    "       .withColumn(\"year\",  year(from_unixtime(\"time\")))\n",
    "       .withColumn(\"month\", month(from_unixtime(\"time\")))\n",
    "       .withColumn(\"day\",   dayofmonth(from_unixtime(\"time\")))\n",
    "      ).repartition(\"year\", \"month\", \"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out different formats and partitioning schemes\n",
    "\n",
    "In all cases, we want to register the result in Glue.\n",
    "\n",
    "First let's stick with JSON to be able to inspect the files externally. The job seems to fail with an error message at the end, after about 5 minutes. The files have all been written to S3, as the next cell shows\n",
    "\n",
    "There are 1198 files, so the data seems complete.\n",
    "\n",
    "If you run the Glue crawler on it, the table gets registered properly. This remains to be investigated further.\n",
    "\n",
    "In any case we want to save the data in a binary, more efficient format, see further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df4.write\n",
    " .option(\"path\", f\"{input_bucket}/raiders-lost-kek/raiders-training-json-part/\")\n",
    " .option(\"mode\", \"overwrite\")\n",
    " .partitionBy(\"year\", \"month\", \"day\")\n",
    " .saveAsTable(\"raiders.raiders_partitioned\", mode='overwrite', format='json')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-11-29 11:16:23   21776251 raiders-lost-kek/raiders-training-parquet-part/year=2019/month=4/day=25/part-00004-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    "2020-11-29 11:16:23   21915954 raiders-lost-kek/raiders-training-parquet-part/year=2019/month=2/day=3/part-00022-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    "2020-11-29 11:16:23   22543995 raiders-lost-kek/raiders-training-parquet-part/year=2018/month=11/day=17/part-00008-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    "2020-11-29 11:16:23   23332997 raiders-lost-kek/raiders-training-parquet-part/year=2019/month=6/day=20/part-00031-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    "2020-11-29 11:16:23   23790847 raiders-lost-kek/raiders-training-parquet-part/year=2016/month=8/day=18/part-00020-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    ".....\n",
    "2020-11-29 11:18:11   22625525 raiders-lost-kek/raiders-training-parquet-part/year=2019/month=10/day=29/part-00902-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    "2020-11-29 11:18:11   24063386 raiders-lost-kek/raiders-training-parquet-part/year=2018/month=9/day=12/part-00897-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    "2020-11-29 11:18:11   24267387 raiders-lost-kek/raiders-training-parquet-part/year=2019/month=5/day=18/part-00899-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    "2020-11-29 11:18:12   31715201 raiders-lost-kek/raiders-training-parquet-part/year=2017/month=1/day=18/part-00890-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    "2020-11-29 11:18:13   25435071 raiders-lost-kek/raiders-training-parquet-part/year=2019/month=8/day=8/part-00890-ae6da587-626b-4f78-ab52-bfea342df229.c000.snappy.parquet\n",
    "2020-11-29 11:18:14          0 raiders-lost-kek/raiders-training-parquet-part/_SUCCESS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet Format\n",
    "\n",
    "Next, we write the same dataframe as a parquet file with snappy compression. It takes about 5 minutes to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df4.write\n",
    " .option(\"path\", f\"{input_bucket}/raiders-lost-kek/raiders-training-parquet-part/\")\n",
    " .partitionBy(\"year\", \"month\", \"day\")\n",
    " .saveAsTable(\"raiders.kek\", mode='overwrite', format='parquet')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code created 1198 files, one for each day with data. The progress bar says 905 tasks, these don't seem to map to partitions then.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT(DATE_TRUNC('day', from_unixtime(time))))\n",
    "FROM raiders.kek\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df4.write\n",
    " .option(\"path\", f\"{input_bucket}/raiders-lost-kek/raiders-training-partitioned/\")\n",
    " .option(\"mode\", \"overwrite\")\n",
    " .saveAsTable(\"raiders.raiders_partitioned\", mode='overwrite')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts, a simple text analysis tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff74967708ad47d2bd5738afe597b77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df5 = df4.select(explode(split(lower(\"com\"), \"[\\W0-9]\")).alias(\"term\"))\n",
    "df6 = df5.groupBy(\"term\").count().sort(\"count\", ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df6.write\n",
    " .option(\"path\", f\"{input_bucket}/raiders-lost-kek/raiders-terms/\")\n",
    " .option(\"mode\", \"overwrite\")\n",
    " .saveAsTable(\"raiders.terms\", mode='overwrite', compression='gzip')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "describe table extended raiders.raiders_partitioned\n",
    "\"\"\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, remove F. prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.select(explode(df.posts).alias(\"posts\"))\n",
    "df3=df2.select(df2.posts.no.alias('number'),\n",
    "              df2.posts.sub.alias('subject'),\n",
    "              df2.posts.country.alias('country'),\n",
    "              from_unixtime(df2.posts.time).alias('time'),\n",
    "              df2.posts.country_name.alias('country_name'),\n",
    "              df2.posts.troll_country.alias('troll_country'),\n",
    "              df2.posts.com.alias('text'),\n",
    "              df2.posts.replies.alias('replies'),\n",
    "              )\n",
    "#df3.show(10)\n",
    "#df3.write.parquet(path=f\"{input_bucket}/raiders-lost-kek/raiders-training-zstd/\", mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.option(\"path\", f\"{input_bucket}/raiders-lost-kek/raiders-training-zstd/\").option(\"compression\", \"zstd\").saveAsTable(\"raiders_zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables in raiders like '^[^p].*'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3://raiders-lost-kek-us-west-2/raiders-lost-kek/raiders-training/\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.parquet(path=f\"{input_bucket}/raiders-lost-kek/raiders-new-from-ori\", mode='overwrite',compression='zstd')\n",
    "#.saveAsTable(\"raiders_new_from_ori\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables in raiders\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.saveAsTable(\"kek\", mode=\"overwrite\", format=\"parquet\", path=f\"{input_bucket}/raiders-lost-kek/raiders_2020/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select country_name, troll_country, count(*)/1e6 as count_in_million\n",
    "from raiders.kek\n",
    "group by 1, 2\n",
    "order by 3 desc;\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with all_posts as\n",
    "(\n",
    "select explode(posts) as cols from raiders.kek\n",
    ")\n",
    "select cols.country, count(*) as count\n",
    "from all_posts\n",
    "group by cols.country\n",
    "order by count desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3://raiders-lost-kek-us-west-2/raiders-lost-kek/raiders-new-from-ori\")\n",
    "sqlContext.registerDataFrameAsTable(df, \"raiders\")\n",
    "spark.sql(\"describe table raiders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select posts.country_name, count(*) as count\n",
    "from raiders\n",
    "where posts.country_name is not null\n",
    "group by 1\n",
    "order by count desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*) as count\n",
    "from raiders\n",
    "where posts.country_name IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables from raiders_2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select(F.explode(df.Records).alias(\"exploded\"))\n",
    "#df2.groupby(\"exploded.eventType\", \"exploded.eventName\").count().sort(\"eventName\", ascending=True).show(250)\n",
    "df3 = (df2.filter(df2.exploded.eventName=='StartQueryExecution')\n",
    "          .filter(df2.exploded.ErrorCode.isNull())\n",
    "          .filter(df2.exploded.requestParameters.queryString.rlike('^SHOW')==False)\n",
    "         .select(df2.exploded.requestParameters.queryString.alias(\"queryString\"),\n",
    "                   df2.exploded.responseElements.queryExecutionId.alias(\"sessionId\"),\n",
    "                   df2.exploded.eventtime.alias(\"startTime\"),\n",
    "                   df2.exploded.eventtime.alias(\"sessionStartTime\"),\n",
    "                   df2.exploded.userIdentity.type.alias(\"userType\"),\n",
    "                   df2.exploded.userIdentity.userName.alias(\"userName\"),\n",
    "                   df2.exploded.userIdentity.principalid.alias(\"principalid\"),\n",
    "          ))\n",
    "df4 = (df3.withColumn(\"defaultDatabases\", F.lit(\"\"))\n",
    "    .withColumn(\"userName\", F.when(df3.userType==\"IAMUSer\", df3.userName).otherwise(df3.userName)))\n",
    "df4.show()\n",
    "df4.write.saveAsTable(\"athena_qli\", mode=\"overwrite\", format=\"parquet\", path=f\"s3://com.alationpro/athena_qli_from_spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_df = df4.toPandas()\n",
    "my_df.to_parquet(path=\"s3://com.alationpro/athena_qli_from_spark_pd/\", egine=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"describe table athena_qli\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "  CASE CAST(useridentity.type AS VARCHAR)\n",
    "  WHEN 'AssumedRole'\n",
    "  THEN SUBSTR(useridentity.principalid, position(':' IN useridentity.principalid)+1)\n",
    "  WHEN 'IAMUser'\n",
    "  THEN useridentity.username\n",
    "  ELSE useridentity.username\n",
    "  END AS userName\n",
    "  ,' ' AS defaultDatabases -- <--- changed from 0 to one space\n",
    "  ,json_extract_scalar(responseElements, '$.queryExecutionId') AS sessionId\n",
    "  ,eventtime AS sessionStartTime\n",
    "  ,eventtime AS startTime\n",
    "  ,json_extract_scalar(requestParameters, '$.queryString') AS queryString\n",
    "  ,'null' AS milliseconds\n",
    "  FROM default.athena_qli\n",
    "  WHERE 1=1\n",
    "  AND region = 'us-west-2'\n",
    "  AND json_extract_scalar(requestParameters, '$.queryString') <> 'SHOW SCHEMAS'\n",
    "  AND json_extract_scalar(requestParameters, '$.queryString') NOT LIKE 'SHOW TABLES IN%'\n",
    "  AND json_extract_scalar(requestParameters, '$.queryString') <> 'SELECT 1'\n",
    "  AND CAST(ErrorCode AS VARCHAR) IS NULL\n",
    "  AND eventname = 'StartQueryExecution'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CloudTrail Logs directly from S3 and extract Athena Query Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c1bfcbba174d6a9ca58196b65b7c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1606733001087_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-27-37.us-west-2.compute.internal:20888/proxy/application_1606733001087_0005/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-17-246.us-west-2.compute.internal:8042/node/containerlogs/container_1606733001087_0005_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "input_bucket = 's3://aws-cloudtrail-logs-255149284406-4cec155e/'\n",
    "input_path = 'AWSLogs/255149284406/CloudTrail/us-west-2/2020/11/30/'\n",
    "df = spark.read.json(input_bucket + input_path)\n",
    "\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dea4115e9d54374b1359117d7e9a231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df2 = df.select(explode(df.Records).alias('exploded'))\n",
    "#df3 = df2.groupby(col(\"exploded.eventName\")).count()\n",
    "#df3.select(col(\"eventName\"), col(\"count\")).sort(\"count\", ascending=False).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fc60651b1148adb2169a8ab60d1b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------+------------+--------------------+----------------+\n",
      "|         queryString|           sessionId|           startTime|    sessionStartTime|userType|    userName|         principalid|defaultDatabases|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------+------------+--------------------+----------------+\n",
      "|select count(*) f...|e4336006-395c-4f2...|2020-11-30T13:36:43Z|2020-11-30T13:36:43Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|select count(*) f...|58ea2ffb-9740-4da...|2020-11-30T13:36:25Z|2020-11-30T13:36:25Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|select count(*) f...|6c296c1f-2ea7-4c1...|2020-11-30T13:36:39Z|2020-11-30T13:36:39Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|MSCK REPAIR TABLE...|c5c7849f-2e55-420...|2020-11-30T10:23:09Z|2020-11-30T10:23:09Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|MSCK REPAIR TABLE...|3d4f269a-2d42-459...|2020-11-30T10:23:21Z|2020-11-30T10:23:21Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|SELECT * \n",
      "FROM Aw...|76c55a4b-34ae-412...|2020-11-30T10:23:46Z|2020-11-30T10:23:46Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|select count(*) f...|7ba59b03-2666-42c...|2020-11-30T13:36:58Z|2020-11-30T13:36:58Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|select count(*) f...|5a9c7348-6bb4-4ce...|2020-11-30T13:36:54Z|2020-11-30T13:36:54Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|SELECT * \n",
      "FROM Aw...|00cb28d7-5b62-420...|2020-11-30T10:20:43Z|2020-11-30T10:20:43Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|SELECT * \n",
      "FROM Aw...|731171f7-4446-417...|2020-11-30T10:22:03Z|2020-11-30T10:22:03Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|select * from ath...|ab1d0153-baca-478...|2020-11-30T12:14:22Z|2020-11-30T12:14:22Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|SELECT COUNT(*) F...|17324039-67d7-48b...|2020-11-30T10:19:58Z|2020-11-30T10:19:58Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|CREATE EXTERNAL T...|a9dc80b6-7c50-452...|2020-11-30T10:19:57Z|2020-11-30T10:19:57Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|CREATE EXTERNAL T...|1095cd10-edce-48d...|2020-11-30T09:55:44Z|2020-11-30T09:55:44Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|SELECT eventName,...|381ef786-32c7-46d...|2020-11-30T09:55:45Z|2020-11-30T09:55:45Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|SELECT * \n",
      "FROM Aw...|3f501c09-4273-47f...|2020-11-30T10:21:32Z|2020-11-30T10:21:32Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|select * from ath...|bab8958f-4afd-4e1...|2020-11-30T10:56:11Z|2020-11-30T10:56:11Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "|SELECT * FROM \"de...|7bac9c7f-044c-4c1...|2020-11-30T10:29:48Z|2020-11-30T10:29:48Z| IAMUser|   MatthiasF|AIDATW2AVDQ3CWYP6...|                |\n",
      "|SELECT * FROM \"de...|d7023628-5d59-4aa...|2020-11-30T10:30:10Z|2020-11-30T10:30:10Z| IAMUser|   MatthiasF|AIDATW2AVDQ3CWYP6...|                |\n",
      "|SELECT * \n",
      "FROM Aw...|7e418fc9-a336-4aa...|2020-11-30T10:24:01Z|2020-11-30T10:24:01Z| IAMUser|matthias_api|AIDATW2AVDQ3A3SRW...|                |\n",
      "+--------------------+--------------------+--------------------+--------------------+--------+------------+--------------------+----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df3 = (df2.filter(df2.exploded.eventName=='StartQueryExecution')\n",
    "          .filter(df2.exploded.ErrorCode.isNull())\n",
    "          .filter(df2.exploded.requestParameters.queryString.rlike('^SHOW')==False)\n",
    "         .select(df2.exploded.requestParameters.queryString.alias(\"queryString\"),\n",
    "                   df2.exploded.responseElements.queryExecutionId.alias(\"sessionId\"),\n",
    "                   df2.exploded.eventtime.alias(\"startTime\"),\n",
    "                   df2.exploded.eventtime.alias(\"sessionStartTime\"),\n",
    "                   df2.exploded.userIdentity.type.alias(\"userType\"),\n",
    "                   df2.exploded.userIdentity.userName.alias(\"userName\"),\n",
    "                   df2.exploded.userIdentity.principalid.alias(\"principalid\"),\n",
    "          ))\n",
    "df4 = (df3.withColumn(\"defaultDatabases\", lit(\"\"))\n",
    "    .withColumn(\"userName\", when(df3.userType==\"IAMUSer\", df3.userName).otherwise(df3.userName)))\n",
    "df4.show()\n",
    "df4.write.saveAsTable(\"athena_qli\", mode=\"overwrite\", format=\"parquet\", path=f\"s3://com.alationpro/athena_qli_from_spark_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9cc32cba5342d7816c4dd2707ca8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          queryString  ... defaultDatabases\n",
      "0                  select count(*) from raiders.terms  ...                 \n",
      "1                    select count(*) from raiders.kek  ...                 \n",
      "2                    select count(*) from raiders.kek  ...                 \n",
      "3   MSCK REPAIR TABLE AwsDataCatalog.default.cloud...  ...                 \n",
      "4         MSCK REPAIR TABLE default.cloudtrail_test_4  ...                 \n",
      "5   SELECT * \\nFROM AwsDataCatalog.default.cloudtr...  ...                 \n",
      "6   select count(*) from awsdatacatalog.raiders.terms  ...                 \n",
      "7     select count(*) from awsdatacatalog.raiders.kek  ...                 \n",
      "8   SELECT * \\nFROM AwsDataCatalog.default.cloudtr...  ...                 \n",
      "9   SELECT * \\nFROM AwsDataCatalog.default.cloudtr...  ...                 \n",
      "10  select * from athena_qli\\norder by starttime desc  ...                 \n",
      "11             SELECT COUNT(*) FROM cloudtrail_test_4  ...                 \n",
      "12  CREATE EXTERNAL TABLE IF NOT EXISTS `cloudtrai...  ...                 \n",
      "13  CREATE EXTERNAL TABLE IF NOT EXISTS default.at...  ...                 \n",
      "14  SELECT eventName,  COUNT(*) \\nfrom default.ath...  ...                 \n",
      "15  SELECT * \\nFROM AwsDataCatalog.default.cloudtr...  ...                 \n",
      "16  select * from athena_qli\\norder by starttime desc  ...                 \n",
      "17  SELECT * FROM \"default\".\"cloudtrail_test_5\" li...  ...                 \n",
      "18  SELECT * FROM \"default\".\"cloudtrail_test_3\" li...  ...                 \n",
      "19  SELECT * \\nFROM AwsDataCatalog.default.cloudtr...  ...                 \n",
      "20  CREATE EXTERNAL TABLE `cloudtrail_test_5`(\\n  ...  ...                 \n",
      "21  SELECT eventname, count(*) \\nFROM default.clou...  ...                 \n",
      "22  CREATE EXTERNAL TABLE `cloudtrail_test_4`(\\n  ...  ...                 \n",
      "23  CREATE EXTERNAL TABLE IF NOT EXISTS `cloudtrai...  ...                 \n",
      "24  -- ALTER TABLE default.cloudtrail_test_3 ADD\\n...  ...                 \n",
      "25  -- ALTER TABLE default.cloudtrail_test_3 ADD\\n...  ...                 \n",
      "26  ALTER TABLE default.cloudtrail_test_3 ADD\\nPAR...  ...                 \n",
      "27     MSCK REPAIR TABLE default.cloudtrail_test_3;\\n  ...                 \n",
      "28  SELECT eventname, count(*) \\nFROM default.clou...  ...                 \n",
      "29  CREATE EXTERNAL TABLE `cloudtrail_test_3`(\\n  ...  ...                 \n",
      "30  SELECT eventname, count(*) \\nFROM default.clou...  ...                 \n",
      "31  ALTER TABLE default.cloudtrail_logs_aws_cloudt...  ...                 \n",
      "32               MSCK REPAIR TABLE cloudtrail_test_5;  ...                 \n",
      "33  SELECT * FROM \"default\".\"cloudtrail_test_5\" li...  ...                 \n",
      "34  -- ALTER TABLE default.cloudtrail_test_3 ADD\\n...  ...                 \n",
      "35         SELECT * FROM default.cloudtrail_test_3;\\n  ...                 \n",
      "36  CREATE EXTERNAL TABLE `cloudtrail_test_1`(\\n  ...  ...                 \n",
      "37  ALTER TABLE default.cloudtrail_test_3 ADD\\nPAR...  ...                 \n",
      "38  -- ALTER TABLE default.cloudtrail_test_3 ADD\\n...  ...                 \n",
      "39  -- ALTER TABLE default.cloudtrail_test_3 ADD\\n...  ...                 \n",
      "40  -- ALTER TABLE default.cloudtrail_test_3 ADD\\n...  ...                 \n",
      "41  SELECT eventname, count(*) \\nFROM default.clou...  ...                 \n",
      "42  SELECT date_trunc('day', eventtime), count(*) ...  ...                 \n",
      "43  SELECT date_trunc('day', eventime), count(*) \\...  ...                 \n",
      "44  CREATE EXTERNAL TABLE `cloudtrail_test_2`(\\n  ...  ...                 \n",
      "45  SELECT date_trunc('day', date(eventtime)), cou...  ...                 \n",
      "46  -- ALTER TABLE default.cloudtrail_test_3 ADD\\n...  ...                 \n",
      "47  SELECT eventname, count(*) \\nFROM default.clou...  ...                 \n",
      "\n",
      "[48 rows x 8 columns]"
     ]
    }
   ],
   "source": [
    "qli = df4.toPandas()\n",
    "qli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7c55564b134b368ea6ff17535a7379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(namespace='default'), Row(namespace='my_database'), Row(namespace='qli'), Row(namespace='raiders'), Row(namespace='sampledb')]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "db = spark.sql(\"SHOW DATABASES\").collect()\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335402ca8bd0403190485b0752495928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'select count(*) from raiders.terms'"
     ]
    }
   ],
   "source": [
    "qli.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def my_replace(row):\n",
    "    print(f\"--{row.index}--\")\n",
    "\n",
    "for d in db:\n",
    "    print(d.namespace)\n",
    "    tables = spark.sql(f\"SHOW TABLES IN {d.namespace}\").collect()\n",
    "    all_tables = [t.tableName for t in tables]\n",
    "    reg_pat = \"|\".join(all_tables)\n",
    "    #res = qli[\"queryString\"].str.extract(f'(?P<catalog>awsdatacatalog)?(?P<schema>{t.database}.)?(?P<table>{all_tables})', flags=re.IGNORECASE)\n",
    "    #res = qli[\"queryString\"].str.extract(f'(?P<schema>{d.namespace}.)(?P<table>{reg_pat})', expand=True,flags=re.IGNORECASE)\n",
    "    res = qli[\"queryString\"].str.extractall(f'(?P<statement>from|join)\\W+(?P<schema>[\\w]+)[.](?P<table>{reg_pat})', flags=re.IGNORECASE)\n",
    "    valid_res = res.loc[pd.notnull(res.table)]\n",
    "    for i, data in valid_res.groupby(level=0):\n",
    "        #print(f\"Need to replace {i.schema}.{i.table} with awsdatacatalog.{d.namespace}.{i.table}\")\n",
    "        #print(qli.loc[i.Index, \"queryString\"])\n",
    "        print(f\"i:{i}, repl:{data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in valid_res.itertuples():\n",
    "    print(i.Index, i.statement, i.schema, i.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=\"raiders\"\n",
    "tables = spark.sql(f\"SHOW TABLES IN {d}\").collect()\n",
    "all_tables = [t.tableName for t in tables]\n",
    "reg_pat = \"|\".join(all_tables)\n",
    "res = qli[\"queryString\"].str.extractall(f'(?P<statement>from|join)\\W+(?P<schema>[\\w]+)[.](?P<table>{reg_pat})', flags=re.IGNORECASE)\n",
    "valid_res = res.loc[pd.notnull(res.table)]\n",
    "#if not valid_res.empty:\n",
    "#    valid_res.apply(my_replace, axis=1)\n",
    "valid_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7811aa6057544502abaab65f2b07e921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/bf/4c/cb7da76f3a5e077e545f9cf8575b8f488a4e8ad60490838f89c5cdd5bb57/pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Collecting python-dateutil>=2.7.3 (from pandas)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.1.4 python-dateutil-2.8.1\n",
      "\n",
      "Collecting pyarrow\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/58/d07e7ee8b0cffe509f9e5a3742e09636a4a58b2113d193166615b934846f/pyarrow-2.0.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.14 in /usr/local/lib64/python3.7/site-packages (from pyarrow)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-2.0.0"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package('pandas')\n",
    "sc.install_pypi_package('pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
